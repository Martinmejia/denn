{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of GAN for Unsupervised deep learning of differential equations\n",
    "\n",
    "Equation:\n",
    "dx/dt = L * x\n",
    "\n",
    "Analytic Solution:\n",
    "x = exp(L * t)\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import tensor, autograd\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_dim=1, out_dim=1, n_hidden_units=20, n_hidden_layers=2, activation=nn.Tanh(),\n",
    "                output_tan=True):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        layers = [('lin1', nn.Linear(in_dim, n_hidden_units)), ('act1', activation)]\n",
    "        for i in range(n_hidden_layers):\n",
    "            layer_id = i+2\n",
    "            layers.append(('lin{}'.format(layer_id), nn.Linear(n_hidden_units, n_hidden_units)))\n",
    "            layers.append(('act{}'.format(layer_id), activation))\n",
    "        layers.append(('linout', nn.Linear(n_hidden_units, out_dim)))\n",
    "        if output_tan:\n",
    "            layers.append(('actout', nn.Tanh()))\n",
    "\n",
    "        layers = OrderedDict(layers)\n",
    "        self.main = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim=1, out_dim=1, n_hidden_units=20, n_hidden_layers=2, activation=nn.Tanh(), unbounded=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        layers = [('lin1', nn.Linear(in_dim, n_hidden_units)), ('act1', activation)]\n",
    "        for i in range(n_hidden_layers):\n",
    "            layer_id = i+2\n",
    "            layers.append(('lin{}'.format(layer_id), nn.Linear(n_hidden_units, n_hidden_units)))\n",
    "            layers.append(('act{}'.format(layer_id), activation))\n",
    "        layers.append(('linout', nn.Linear(n_hidden_units, out_dim)))\n",
    "        if not unbounded:\n",
    "            # unbounded used for WGAN (no sigmoid)\n",
    "            layers.append(('actout', nn.Sigmoid()))\n",
    "\n",
    "        layers = OrderedDict(layers)\n",
    "        self.main = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output\n",
    "\n",
    "def realtime_vis(g_loss, d_loss, t, preds, analytic_fn, dx_dt, d2x_dt2, savefig=False, fname=None):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(20,6))\n",
    "    steps = len(g_loss)\n",
    "    epochs = np.arange(steps)\n",
    "\n",
    "    ax[0].plot(epochs, d_loss, label='d_loss')\n",
    "    ax[0].plot(epochs, g_loss, label='g_loss')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title('Losses')\n",
    "\n",
    "    ax[1].plot(t, analytic_fn(t), label='true')\n",
    "    ax[1].plot(t, preds, '--', label='pred')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title('X Pred')\n",
    "\n",
    "    ax[2].plot(t, dx_dt, label='dx_dt')\n",
    "    ax[2].plot(t, d2x_dt2, label='d2x_dt2')\n",
    "    ax[2].plot(t, preds, '--', label='x')\n",
    "    ax[2].legend()\n",
    "    ax[2].set_title('Derivatives')\n",
    "\n",
    "    if not savefig:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(fname)\n",
    "\n",
    "def diff(x, t):\n",
    "    dx_dt, = autograd.grad(x, t,\n",
    "                           grad_outputs=x.data.new(x.shape).fill_(1),\n",
    "                           create_graph=True)\n",
    "    return dx_dt\n",
    "\n",
    "def train_GAN_SHO(\n",
    "                num_epochs,\n",
    "                activation=nn.Tanh(),\n",
    "                g_hidden_units=10,\n",
    "                d_hidden_units=10,\n",
    "                g_hidden_layers=2,\n",
    "                d_hidden_layers=2,\n",
    "                d_lr=0.001,\n",
    "                g_lr=0.001,\n",
    "                d_betas=(0.9, 0.999),\n",
    "                g_betas=(0.9, 0.999),\n",
    "                G_iters=1,\n",
    "                D_iters=1,\n",
    "    \n",
    "                t_low=0,\n",
    "                t_high=10,\n",
    "                x0=0,\n",
    "                dx_dt0=.5,\n",
    "                n=100,\n",
    "                m=1.,\n",
    "                k=1.,\n",
    "                real_label=1,\n",
    "                fake_label=0,\n",
    "                \n",
    "                logging=True,\n",
    "                realtime_plot=False,\n",
    "                \n",
    "                soft_labels=False,\n",
    "                real_data=False,\n",
    "                loss_diff=.1,\n",
    "                max_while=20,\n",
    "                \n",
    "                wgan=False,\n",
    "                clip=.1,\n",
    "                gradient_penalty=False,\n",
    "                gp_hyper=0.1,\n",
    "                \n",
    "                systemOfODE=False,\n",
    "                savefig=False,\n",
    "                fname=None):\n",
    "\n",
    "    \"\"\"\n",
    "    function to perform training of generator and discriminator for num_epochs\n",
    "    equation: simple harmonic oscillator (SHO)\n",
    "    gan hacks:\n",
    "        - wasserstein + clipping / wasserstein GP\n",
    "        - label smoothing\n",
    "        - while loop iters\n",
    "    \"\"\"\n",
    "    if savefig and realtime_plot:\n",
    "        raise Exception('savefig and realtime_plot both True. Assuming you dont want that.')\n",
    "\n",
    "    if wgan:\n",
    "        fake_label = -1\n",
    "\n",
    "    # initialize nets\n",
    "    G = Generator(in_dim=1, out_dim=1,\n",
    "                  n_hidden_units=g_hidden_units,\n",
    "                  n_hidden_layers=g_hidden_layers,\n",
    "                  activation=activation, # twice diff'able activation\n",
    "                  output_tan=True) # output range should be (-1,1) if True\n",
    "\n",
    "    D = Discriminator(in_dim=1, out_dim=1,\n",
    "                      n_hidden_units=d_hidden_units,\n",
    "                      n_hidden_layers=d_hidden_layers,\n",
    "                      activation=activation,\n",
    "                      unbounded=wgan) # true for WGAN\n",
    "\n",
    "    # grid\n",
    "    t_torch = torch.linspace(t_low, t_high, n, dtype=torch.float, requires_grad=True).reshape(-1,1)\n",
    "    t_np = np.linspace(t_low, t_high, n).reshape(-1,1)\n",
    "\n",
    "    delta_t = t_torch[1]-t_torch[0]\n",
    "    def get_batch():\n",
    "        \"\"\" perturb grid \"\"\"\n",
    "        return t_torch + delta_t * torch.randn_like(t_torch) / 3\n",
    "\n",
    "    # labels\n",
    "    real_label_vec = torch.full((n,), real_label).reshape(-1,1)\n",
    "    fake_label_vec = torch.full((n,), fake_label).reshape(-1,1)\n",
    "\n",
    "    # optimization\n",
    "    if wgan:\n",
    "        criterion = lambda y_true, y_pred: torch.mean(y_true * y_pred)\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "    optiG = torch.optim.Adam(G.parameters(), lr=g_lr, betas=g_betas)\n",
    "    optiD = torch.optim.Adam(D.parameters(), lr=d_lr, betas=d_betas)\n",
    "\n",
    "    # logging\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "\n",
    "    analytic_oscillator = lambda t: x0*torch.cos(t) + dx_dt0*torch.sin(t)\n",
    "    analytic_oscillator_np = lambda t: x0*np.cos(t) + dx_dt0*np.sin(t)\n",
    "\n",
    "    def produce_SHO_preds(G, t):\n",
    "        x_raw = G(t)\n",
    "        \n",
    "        # adjust for initial conditions on x and dx_dt\n",
    "        x_adj = x0 + (1 - torch.exp(-t)) * dx_dt0 + ((1 - torch.exp(-t))**2) * x_raw\n",
    "\n",
    "        dx_dt = diff(x_adj, t)\n",
    "        d2x_dt2 = diff(dx_dt, t)\n",
    "\n",
    "        return x_adj, dx_dt, d2x_dt2\n",
    "\n",
    "    def produce_SHO_preds_system(G, t):\n",
    "        x_pred = G(t)\n",
    "        \n",
    "        # x condition\n",
    "        x_adj = x0 + (1 - torch.exp(-t)) * dx_dt0 + ((1 - torch.exp(-t))**2) * x_pred\n",
    "        \n",
    "        # dx_dt\n",
    "        dx_dt = diff(x_pred, t)\n",
    "        \n",
    "        # u condition guarantees that dx_dt = u (first equation in system)\n",
    "        u_adj = torch.exp(-t) * dx_dt0 + 2 * (1 - torch.exp(-t)) * torch.exp(-t) * x_pred + (1 - torch.exp(-t)) * dx_dt\n",
    "        \n",
    "        # compute du_dt = d2x_dt2\n",
    "        du_dt = diff(u_adj, t)\n",
    "\n",
    "        return x_adj, u_adj, du_dt\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        ## =========\n",
    "        ##  TRAIN G\n",
    "        ## =========\n",
    "\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = False # turn off computation for D\n",
    "\n",
    "        t = get_batch()\n",
    "        if real_data:\n",
    "            real = analytic_oscillator(t)\n",
    "\n",
    "        for i in range(G_iters):\n",
    "\n",
    "            if systemOfODE:\n",
    "                x_adj, u_adj, du_dt = produce_SHO_preds_system(G, t)\n",
    "                d2x_dt2 = du_dt\n",
    "                \n",
    "            else:\n",
    "                x_adj, dx_dt, d2x_dt2 = produce_SHO_preds(G, t)\n",
    "\n",
    "            if real_data:\n",
    "                fake = x_adj\n",
    "            else:\n",
    "                real = x_adj\n",
    "                fake = -(m/k)*d2x_dt2\n",
    "\n",
    "            # generator loss\n",
    "            g_loss = criterion(D(fake), real_label_vec)\n",
    "\n",
    "            optiG.zero_grad() # zero grad before backprop\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            if wgan and not gradient_penalty:\n",
    "                g_grad_norm = nn.utils.clip_grad_norm_(G.parameters(), clip)\n",
    "            optiG.step()\n",
    "\n",
    "        ## =========\n",
    "        ##  TRAIN D\n",
    "        ## =========\n",
    "\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = True # turn on computation for D\n",
    "\n",
    "        for i in range(D_iters):\n",
    "\n",
    "            if soft_labels:\n",
    "                real_label_vec_ = real_label_vec + (-.2 + .4 * torch.rand_like(real_label_vec))\n",
    "                fake_label_vec_ = fake_label_vec + (-.2 + .4 * torch.rand_like(fake_label_vec))\n",
    "            else:\n",
    "                real_label_vec_ = real_label_vec\n",
    "                fake_label_vec_ = fake_label_vec\n",
    "\n",
    "            ## WGAN - GP penalty\n",
    "            norm_penalty = torch.zeros(1)\n",
    "            \n",
    "            if wgan and gradient_penalty:\n",
    "                total_norm = torch.zeros(1)\n",
    "\n",
    "                eps_mix = torch.rand(real.shape[0]) \n",
    "                x_mix = torch.zeros_like(eps_mix)\n",
    "                \n",
    "                for i,eps in enumerate(eps_mix):\n",
    "                    x_mix[i] = eps * real[i] + (1-eps) * fake[i]\n",
    "\n",
    "                x_mix = x_mix.reshape(-1,1)\n",
    "\n",
    "                mix_preds = D(x_mix)\n",
    "                mix_grad = diff(mix_preds, x_mix)\n",
    "                \n",
    "\n",
    "                total_norm = torch.pow(mix_grad, 2) ** (1. / 2)\n",
    "                \n",
    "                norm_penalty = torch.mean(gp_hyper * torch.pow(total_norm - 1, 2))\n",
    "\n",
    "            # discriminator loss\n",
    "            real_loss = criterion(D(real), real_label_vec_)\n",
    "            fake_loss = criterion(D(fake), fake_label_vec_)\n",
    "            \n",
    "            d_loss = real_loss + fake_loss + norm_penalty\n",
    "            # d_loss = torch.mean(D(fake) - D(real) + norm_penalty)\n",
    "            \n",
    "            optiD.zero_grad()\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            if wgan and not gradient_penalty:\n",
    "                d_grad_norm = nn.utils.clip_grad_norm_(D.parameters(), clip)\n",
    "            optiD.step()\n",
    "\n",
    "        ## ========\n",
    "        ## Logging\n",
    "        ## ========\n",
    "\n",
    "        if logging:\n",
    "            print('[%d/%d] D_Loss : %.4f Loss_G: %.4f' % (epoch, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "        D_losses.append(d_loss.item())\n",
    "        G_losses.append(g_loss.item())\n",
    "\n",
    "        if (realtime_plot or epoch == num_epochs - 1):\n",
    "            # either every time or on last epoch, show plots\n",
    "            # if savefig is True, the figure will be saved\n",
    "            # (only on last epoch), because we make sure both are not true\n",
    "            \n",
    "            clear_output(True)\n",
    "            if not real_data:\n",
    "                \n",
    "                if systemOfODE:\n",
    "                    \n",
    "                    x_adj, u_adj, du_dt = produce_SHO_preds_system(G, t)\n",
    "                    d2x_dt2 = du_dt\n",
    "                    realtime_vis(G_losses, D_losses, t_np, x_adj.detach().numpy(), analytic_oscillator_np,\n",
    "                                u_adj.detach().numpy(), d2x_dt2.detach().numpy(), savefig=savefig, fname=fname)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    x_adj, dx_dt, d2x_dt2 = produce_SHO_preds(G, t_torch)\n",
    "                    realtime_vis(G_losses, D_losses, t_np, x_adj.detach().numpy(), analytic_oscillator_np,\n",
    "                                dx_dt.detach().numpy(), d2x_dt2.detach().numpy(), savefig=savefig, fname=fname)\n",
    "                    \n",
    "            else:\n",
    "                loss_ax, pred_ax = plot_losses_and_preds(G_losses, D_losses, G, t_np, analytic_oscillator_np)\n",
    "                plt.show()\n",
    "\n",
    "    return G, D, G_losses, D_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(42)\n",
    "G,D,G_loss,D_loss = train_GAN_SHO(\n",
    "    \n",
    "                              # EPOCHS\n",
    "                              16666, \n",
    "                                      \n",
    "                              # NETWORKS\n",
    "                              activation=nn.Tanh(),\n",
    "                              g_hidden_units=80, \n",
    "                              g_hidden_layers=8,\n",
    "                              d_hidden_units=20, \n",
    "                              d_hidden_layers=2,\n",
    "                              \n",
    "                              G_iters=1,\n",
    "                              D_iters=5,\n",
    "                              \n",
    "                              # FROM WGAN PAPER\n",
    "                              d_lr=0.0001,\n",
    "                              g_lr=0.0001,\n",
    "                              d_betas=(0., 0.9),\n",
    "                              g_betas=(0., 0.9),\n",
    "                                      \n",
    "                              # PROBLEM      \n",
    "                              t_low=0,\n",
    "                              t_high=2*np.pi,\n",
    "                              n=200,\n",
    "                              x0=0.,\n",
    "                              dx_dt0=.5,\n",
    "                                 \n",
    "                              # VIZ\n",
    "                              logging=False,\n",
    "                              realtime_plot=False,\n",
    "                              \n",
    "                              # Hacks\n",
    "                              real_data=False,                                      \n",
    "                              soft_labels=False,\n",
    "                              \n",
    "                              # WGAN\n",
    "                              wgan=True,\n",
    "                              gradient_penalty=True,\n",
    "                              gp_hyper=10.,\n",
    "                              \n",
    "                              # SYSTEM\n",
    "                              systemOfODE=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
